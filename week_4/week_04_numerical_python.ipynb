{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7b9a71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap of Week 3: Code structuring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32048f7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the last week we continued to learn basic concepts of Python. In particular, we learned how to organise code into smaller components for easier re-use, testing and sharing. We covered the following topics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535d165d-7e65-4886-b4f4-b7c077d9c931",
   "metadata": {},
   "source": [
    "- How to make you life easier with string formatting\n",
    "- Using the Python standard library to find and use useful functions\n",
    "- Ways of bundling up your code into reusable units with functions\n",
    "- Making it possible to share your code with others by moving code into modules\n",
    "- How to produce custom errors\n",
    "- How to compactly generate lists with list comprehensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4a11d6-fd3b-48fd-96c1-399c21f2f58c",
   "metadata": {},
   "source": [
    "We covered a wide range of the core concepts of the Python programming language, including data types, slicing, loops, branching, functions, modules and errors, in the last two weeks. Now, we can finally apply all of this to start processing scientific data. We will still learn about new ideas and tools along the way, but you will see that nearly all of these concepts - and the syntax to apply them - will be based upon the fundamental building blocks we learned about over the last two weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e240bc-160d-489c-8a12-bd680d09f8f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 4: Numerical Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09afa9b2-cba2-43f6-ad65-7b834319a5b7",
   "metadata": {},
   "source": [
    "This week we will learn how to easily and efficiently work with large collections of numerical data. This could be a timeseries of observations, output from your environmental model or a distributed collection of multi-layer satellite data. The premier tool to work with any of these datasets is called NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5f95b-b9d8-4575-9a5f-a059f316fd97",
   "metadata": {},
   "source": [
    "## What is NumPy?\n",
    "\n",
    "NumPy (short for Numerical Python and pronouced *num-pea* or *num-pie*) is a third-party Python module for numerical programming. Any time you have collections of numbers, either one-, two- or more dimensional, then NumPy is likely to help you out. It provides you with a large suite of tools, algorithms and techniques. It is one of the most commonly used Python packages around and is used in the majority of Python-based scientific software."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fc48fb-1652-4c05-88a0-615435c453cf",
   "metadata": {},
   "source": [
    "## Motivation: the speed of NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63761458-ef4b-4765-a591-9cc2495ed356",
   "metadata": {},
   "source": [
    "Before we start actually introducing NumPy, let's use an easy example to demonstrate the main reason why NumPy is so popular and widely used in scientific computing: it is extremely fast! \n",
    "\n",
    "Let's consider we want to add together all the integers from 1 to 1000000. We could do this with the tools we learned so far:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae31ecff-c15d-4f0d-884c-8d87525fce01",
   "metadata": {},
   "source": [
    "### So far: data processing with lists\n",
    "We can loop over a list of numbers and add each of them to a total count like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e5db4908-a425-4084-99a6-def14f458297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time in seconds: 0.07615804672241211\n"
     ]
    }
   ],
   "source": [
    "# we load the time module here to measure the time our code needs to run\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "total = 0\n",
    "for num in range(1000001):\n",
    "    total += num\n",
    "\n",
    "elapsed_time_list = time.time() - start_time\n",
    "print(f\"time in seconds: {elapsed_time_list}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f40ba-b981-4079-a949-f2c4576ad980",
   "metadata": {},
   "source": [
    "### New: the NumPy way\n",
    "Let's do the same calculation with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce1ed7d8-6ddb-4672-b1bd-72ca6f522f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time in seconds: 0.0029807090759277344\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "numpy_total = np.sum(np.arange(1000001))\n",
    "\n",
    "elapsed_time_numpy = time.time() - start_time\n",
    "\n",
    "print(f\"time in seconds: {elapsed_time_numpy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a6c6a5cc-58a4-4bbb-95a3-cdc73bcd3e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy speed-up: 26.032074868021116\n"
     ]
    }
   ],
   "source": [
    "print(f\"NumPy speed-up: {elapsed_time_list / elapsed_time_numpy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c157840d-6b8a-47b2-a432-19464acf23c5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We see that NumPy can be an order of magnitude faster in processing large collections of data than anything we could do before (we will see why this is later on). Obviously, the amounts of data you will deal with when running environmental models or analysing geographical data sets in the future can get very big, very fast. So today, let's learn more about NumPy and how we can use it to unlock completely new ways of working with large, multi-dimensional datasets.\n",
    "\n",
    "You can jump ahead to any chapter with the navigation below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5203ca7-14fa-4e5d-b023-54d379b666f7",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. #### [String formatting](./chapters/01-string_formatting.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706c2e6b-53ca-4d31-8cac-733389b903b7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## [[Next](./chapters/01-string_formatting.ipynb)]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
